{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3ea87c",
   "metadata": {},
   "source": [
    "# 10 — Data Loaders (Introduction)\n",
    "\n",
    "In LangChain, **Data Loaders** are components that **fetch data from external sources** (files, databases, APIs, websites, private docs, etc.) and convert it into a format that an LLM can consume.\n",
    "\n",
    "They:\n",
    "- Connect to the data source.\n",
    "- Transform the raw content into **documents** usable as context.\n",
    "- Enable **context-aware** Q&A and workflows.\n",
    "\n",
    "This is crucial when you want your model to use **domain-specific** or **private** information rather than only general knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f17fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment loaded and model ready.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════╗\n",
    "# ║ Setup: Load environment variables & initialize model ║\n",
    "# ╚══════════════════════════════════════════════════════╝\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# chat_model = ChatGroq(model=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "print(\"✅ Environment loaded and model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c79096",
   "metadata": {},
   "source": [
    "## Why Data Loaders matter\n",
    "\n",
    "- **Augment knowledge** with your own sources (PDFs, spreadsheets, DBs, CRMs).\n",
    "- **Reduce hallucinations** by providing reliable context.\n",
    "- **Automate ingestion** from many systems with minimal code.\n",
    "\n",
    "Common sources include: PDFs/Word/Excel, SQL/NoSQL, Google Drive, Notion, Slack, websites/APIs, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b5eb8",
   "metadata": {},
   "source": [
    "## Practical example — WikipediaLoader\n",
    "\n",
    "We will fetch a single page from Wikipedia and use its text as **context** for a question. \n",
    "Note: this requires internet access and the `wikipedia` dependency used by the loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35505124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory. His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called \"the world's most famous equation\". He received the 1921 Nobel Prize in Physics for \"his services t\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a page from Wikipedia\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "person = \"Einstein\"  # try changing this to another public figure\n",
    "loader = WikipediaLoader(query=person, load_max_docs=1)\n",
    "\n",
    "docs = loader.load()\n",
    "assert len(docs) > 0, \"No document returned by WikipediaLoader.\"\n",
    "loaded_text = docs[0].page_content\n",
    "\n",
    "print(loaded_text[:400] + \"\\n...\\n\")  # preview first chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd504e0f",
   "metadata": {},
   "source": [
    "## Use the loaded data as context in a prompt\n",
    "\n",
    "We'll build a small chat prompt that accepts a **question** and **context**, then ask the model to answer concisely based only on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c6e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Answer the following question using ONLY the provided context.\\n\\n\"\n",
    "            \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer concisely.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "question = \"What was the full name of Einstein?\"\n",
    "print(chain.invoke({\"question\": question, \"context\": loaded_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64bf89",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "1. **WikipediaLoader** fetched the page and returned a list of `Document` objects.\n",
    "2. We extracted `page_content` from the first document.\n",
    "3. We constructed a **chat prompt** with placeholders for `question` and `context`.\n",
    "4. We ran `prompt → model → parser` to get a clean, concise text answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b8f03",
   "metadata": {},
   "source": [
    "## Notes and good practices\n",
    "\n",
    "- Context can be **very long**; later we will split it into chunks and do retrieval (RAG). For now we feed the raw text.\n",
    "- Keep questions precise and tell the model to rely on **context only** to reduce hallucinations.\n",
    "- For private data (PDFs/DBs/Drive/etc.), use the corresponding loaders in `langchain_community.document_loaders`.\n",
    "- Always check and handle empty results (e.g., missing page or network issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c8737",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Data Loaders** bring external information into your LLM workflows.\n",
    "- They let you **ground** model answers in reliable sources.\n",
    "- In the next RAG notebooks, we’ll split documents, build embeddings, and retrieve the **most relevant** chunks before generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
