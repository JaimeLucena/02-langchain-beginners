{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 04 â€” Loading Environment Variables & Basic Model Calls\n",
    "\n",
    "Before building apps with LangChain, letâ€™s start with the **basics**:\n",
    "- Load API keys from the `.env` file.\n",
    "- Instantiate a model (OpenAI or Groq).\n",
    "- Ask your first question.\n",
    "- Understand different call methods: `.invoke()`, `.batch()`, `.stream()`, `.ainvoke()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dotenv-section",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Step 1: Load environment variables\n",
    "\n",
    "The `.env` file in your repo root contains your API keys. Weâ€™ll load it using the `python-dotenv` package.\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=ADD_KEY_HERE\n",
    "GROQ_API_KEY=ADD_KEY_HERE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-dotenv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded.\n",
      "OpenAI key detected: True\n",
      "Groq key detected: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load .env file if present\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Retrieve the API key(s)\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "print(\"âœ… Environment loaded.\")\n",
    "print(\"OpenAI key detected:\", bool(openai_api_key))\n",
    "print(\"Groq key detected:\", bool(groq_api_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instantiate-model",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Step 2: Instantiate a model\n",
    "\n",
    "LangChain provides wrappers for many LLM providers.\n",
    "\n",
    "Here, weâ€™ll use **OpenAI** for simplicity. (You could easily switch to Groq by changing the import and model name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instantiate-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a lightweight model instance\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "print(\"âœ… Model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invoke",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Step 3: Ask your first question with `.invoke()`\n",
    "\n",
    "The simplest way to query a model is with `.invoke(input)`.\n",
    "\n",
    "This performs a **synchronous** (blocking) call and returns the full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invoke-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"Explain what LangChain is in one sentence.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Step 4: Multiple questions with `.batch()`\n",
    "\n",
    "You can send multiple inputs in one batch. LangChain will handle them efficiently and return a list of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is a token in LLMs?\",\n",
    "    \"Define what a prompt template is.\",\n",
    "    \"Explain what 'context window' means.\",\n",
    "]\n",
    "\n",
    "responses = model.batch(questions)\n",
    "for i, r in enumerate(responses, 1):\n",
    "    print(f\"[{i}] {r.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stream",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Step 5: Streaming responses with `.stream()`\n",
    "\n",
    "Streaming returns chunks of the modelâ€™s output as they are generated â€” ideal for chat interfaces.\n",
    "\n",
    "You can iterate over `.stream()` like a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stream-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to facilitate the development of applications that utilize language models. Here are three practical uses of LangChain:\n",
      "\n",
      "1. **Conversational Agents and Chatbots**: LangChain can be used to build sophisticated conversational agents that can understand and respond to user queries in natural language. By leveraging the capabilities of language models, developers can create chatbots for customer support, virtual assistants, or interactive storytelling applications that provide engaging and context-aware interactions.\n",
      "\n",
      "2. **Document Analysis and Summarization**: LangChain can assist in processing and analyzing large volumes of text documents. It can be used to extract key information, summarize content, or generate insights from documents such as research papers, legal contracts, or business reports. This can help users quickly grasp essential information without having to read through entire documents.\n",
      "\n",
      "3. **Personalized Content Generation**: LangChain can be employed to create personalized content for users based on their preferences and behavior. For example, it can generate tailored marketing emails, product recommendations, or social media posts that resonate with individual users, enhancing engagement and improving conversion rates.\n",
      "\n",
      "These applications demonstrate the versatility of LangChain in various domains, from customer service to content creation and data analysis.\n"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"List three practical uses of LangChain.\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "switch-groq",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Optional: Using Groq instead of OpenAI\n",
    "\n",
    "If you prefer an open-source model, you can switch to **Groq**. It uses a very similar interface:\n",
    "\n",
    "```python\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_model = ChatGroq(model=\"llama-3.1-70b-versatile\", temperature=0.2)\n",
    "response = groq_model.invoke(\"Explain what an embedding is.\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "Both providers can coexist; choose one depending on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "- `.env` â†’ stores your API keys.\n",
    "- `dotenv` â†’ loads them automatically into environment variables.\n",
    "- `ChatOpenAI` and `ChatGroq` â†’ model clients.\n",
    "- `.invoke()` â†’ one call, one result.\n",
    "- `.batch()` â†’ multiple calls in parallel.\n",
    "- `.stream()` â†’ live output.\n",
    "- `.ainvoke()` â†’ async non-blocking calls.\n",
    "\n",
    "You now have all the basics to start building and testing LLM apps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
