{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa04fc10",
   "metadata": {},
   "source": [
    "# 09 — Composing Chains (Using One Chain's Output as Another's Input)\n",
    "\n",
    "In LCEL, you can use the **output of one chain as the input of another**. This lets you build pipelines where one sub-problem feeds the next (e.g., *politician → country* and then *country → continent*).\n",
    "\n",
    "You compose chains with the `|` operator and you can place **subchains** inside parallel dict blocks to fan-out values that will later be fanned-in by a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19efd0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment loaded and model ready.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════╗\n",
    "# ║ Setup: Load environment variables & initialize model ║\n",
    "# ╚══════════════════════════════════════════════════════╝\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# chat_model = ChatGroq(model=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "print(\"✅ Environment loaded and model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04141d2",
   "metadata": {},
   "source": [
    "## Example 1 — Any composed chain (context)\n",
    "\n",
    "A composed chain is simply a sequence like **prompt → model → parser**:\n",
    "\n",
    "```python\n",
    "# Illustrative example (not executed here):\n",
    "# composed_chain.invoke({\"politician\": \"Attila\"})\n",
    "# → \"Attila the Hun's conquests... rather than positive contributions to humanity.\"\n",
    "```\n",
    "\n",
    "Nothing new here: chains can be **composed in series** with `|`. The important part is that the **final output** can then be used by another chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93207cbb",
   "metadata": {},
   "source": [
    "## Example 2 — A chain inside another (dependent fan-in)\n",
    "\n",
    "**Goal:**\n",
    "1) Ask the **country** of a politician.\n",
    "2) Using that country, ask the **continent**, in a given language.\n",
    "\n",
    "We will build two subchains and nest the first one inside a parallel dict block that also passes the language through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4787cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le pays dont François Mitterrand était originaire, la France, se trouve en Europe.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"What country is {politician} from?\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"What continent is the country {country} in? Respond in {language}.\"\n",
    ")\n",
    "\n",
    "# Chain 1: (politician) -> country (as plain string)\n",
    "chain1 = prompt1 | chat_model | StrOutputParser()\n",
    "\n",
    "# Chain 2: use chain1's output as the 'country' variable and also pass 'language'\n",
    "chain2 = (\n",
    "    {\n",
    "        \"country\": chain1,                 # subchain runs with the SAME input (needs 'politician')\n",
    "        \"language\": itemgetter(\"language\") # pass 'language' directly from the incoming dict\n",
    "    }\n",
    "    | prompt2\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\n",
    "    chain2.invoke({\"politician\": \"François Mitterrand\", \"language\": \"French\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08492c9a",
   "metadata": {},
   "source": [
    "### What’s happening (step by step)\n",
    "\n",
    "1. **Input:** `{\"politician\": \"...\", \"language\": \"...\"}`.\n",
    "2. The parallel dict runs **both keys**:\n",
    "   - `\"country\": chain1` → executes `chain1` with the **same input** (it uses `politician`) and returns a country string.\n",
    "   - `\"language\": itemgetter(\"language\")` → extracts `language` unchanged from the input.\n",
    "3. A new dict is formed: `{\"country\": <country>, \"language\": <language>}` which feeds `prompt2`.\n",
    "4. `prompt2 → model → parser` produce the final answer (in the requested language)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276dba5f",
   "metadata": {},
   "source": [
    "## Remember what `itemgetter` does\n",
    "\n",
    "`itemgetter` (from Python’s `operator` module) returns a function that pulls a specific key from a dict — perfect as a tiny wiring tool inside chains.\n",
    "\n",
    "```python\n",
    "from operator import itemgetter\n",
    "extract_language = itemgetter(\"language\")\n",
    "extract_language({\"language\": \"French\", \"other\": 123})  # → \"French\"\n",
    "```\n",
    "\n",
    "Use it whenever you want to **route** one field from the input to a chain stage **without modifying it**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16f122",
   "metadata": {},
   "source": [
    "## Patterns to learn from this design\n",
    "\n",
    "- **Subchains as nodes:** any runnable can be a value inside a dict; the **subchain’s output** becomes the value for that key.\n",
    "- **`itemgetter` as a cable:** forwards fields from the input verbatim.\n",
    "- **Consistent types:** if `prompt2` expects `country` as a **string**, ensure `chain1` outputs a **string** (hence the `StrOutputParser`).\n",
    "- **Fan-out → Fan-in:** the dict creates a **fan-out** (country/language). The prompt then **fans in** (consumes) those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5dcbf8",
   "metadata": {},
   "source": [
    "## Best practices\n",
    "\n",
    "- **Fix output types early** (e.g., `StrOutputParser`, JSON parsers) to avoid downstream mismatches.\n",
    "- **Name dict keys clearly** to match the variables consumed later (`country`, `language`, etc.).\n",
    "- **Reuse subchains** — define once, use many times.\n",
    "- **Validate assumptions** — LLMs can return noisy strings (e.g., *\"France (Europe)\"*). If you need exactness, post-process (regex/normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e890dab",
   "metadata": {},
   "source": [
    "## Useful variants\n",
    "\n",
    "- **With `.assign()`** — build a base dict and then **add** derived keys:\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "base = {\"country\": chain1, \"language\": itemgetter(\"language\")}\n",
    "vars_dict = RunnableParallel(base).assign(\n",
    "    meta=RunnableLambda(lambda d: {\"country_len\": len(d[\"country\"])})\n",
    ")\n",
    "final = vars_dict | prompt2 | chat_model | StrOutputParser()\n",
    "```\n",
    "\n",
    "- **With `RunnableParallel` explicitly** — when you want to emphasize the fan-out stage.\n",
    "- **With `RunnableBranch`** — if the subchain depends on conditions (e.g., choose different prompts by language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e6598-d475-4fb1-9cc7-a1313b5a1528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
