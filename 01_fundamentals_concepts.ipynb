{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 01 ‚Äî Context, Prompt, and Tokens (LLM Fundamentals)\n",
    "\n",
    "This notebook explains the key concepts to understand how Large Language Models (LLMs) think and respond: **context**, **context window**, **tokens**, **prompts**, and **hallucinations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "**Context** is the information an AI app or chatbot takes into account to generate an answer.\n",
    "\n",
    "### üìå Two types of context\n",
    "1. **Conversational context** ‚Üí what you and the chatbot have discussed so far (the ‚Äúcontext window‚Äù).\n",
    "2. **External or extended context** ‚Üí additional information the app connects to the model, such as:\n",
    "   - **Databases** (e.g., inventory, CRM, customer records).\n",
    "   - **Documents** (e.g., PDFs, manuals, contracts).\n",
    "   - **External APIs** (e.g., calendar, weather, search).\n",
    "\n",
    "### üìå Key difference\n",
    "- **Prompt** ‚Üí what you write (the instruction or question you give the chatbot).\n",
    "- **Context** ‚Üí all the information the model considers to answer (includes your prompt, the prior chat, system instructions, attached documents, etc.).\n",
    "\n",
    "### üìå Example\n",
    "- You ask in an AI app: *‚ÄúWhat is the status of order #12345?‚Äù*\n",
    "- The **prompt** is your question.\n",
    "- The **conversational context** includes your prior messages.\n",
    "- The **external context** could be that the app queries an **orders database** and passes those results to the model so it can answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-window",
   "metadata": {},
   "source": [
    "## Context Window\n",
    "\n",
    "The **context window** is the model‚Äôs **short-term memory**.\n",
    "\n",
    "- The model does not remember everything forever, only what **fits** in this window.\n",
    "- The window is measured in **tokens** (chunks of text).\n",
    "- If the conversation grows too long and exceeds the limit, the oldest parts are **truncated** (they get ‚Äúforgotten‚Äù).\n",
    "\n",
    "**Rough example:**\n",
    "A model with a **context window of 8,000 tokens** can handle roughly **20‚Äì25 pages** of text before it starts ‚Äúforgetting‚Äù the earliest content. (It depends on language, formatting, and content.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokens",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "A **token** is a chunk of text (it can be a whole word or part of a word).\n",
    "\n",
    "- ‚Äúcat‚Äù ‚Üí ~1 token.\n",
    "- ‚Äúextraordinarily‚Äù ‚Üí multiple tokens.\n",
    "- In English, ‚Äúplaying‚Äù might split into ‚Äúplay‚Äù + ‚Äúing‚Äù.\n",
    "\n",
    "Models process **tokens**, not individual letters or full words.\n",
    "\n",
    "**Useful mental rule of thumb:**\n",
    "If a model has a limit of **4,000 tokens**, that‚Äôs roughly ~**3,000 words**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "A **prompt** is the instruction or input you give to the model.\n",
    "\n",
    "- It can be a question (*‚ÄúExplain what machine learning is.‚Äù*).\n",
    "- Or a set of rules (*‚ÄúAct as a math teacher and use simple examples.‚Äù*).\n",
    "\n",
    "A good prompt typically:\n",
    "- Clarifies the **role** (who the model should be).\n",
    "- Defines the **goal** (what it should achieve).\n",
    "- Specifies the **output format** (list, JSON, table, etc.).\n",
    "- States **quality criteria** (concise, no jargon, include examples, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hallucinations",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "\n",
    "**Hallucinations** are **made-up errors** from the model.\n",
    "\n",
    "- Sometimes, to avoid giving no answer, the model **invents** something that sounds plausible.\n",
    "- Example: if you ask ‚ÄúWho won the Football World Cup in 2025?‚Äù (hasn‚Äôt happened yet), the model might make up a winner because it tries to produce a plausible answer.\n",
    "\n",
    "**Ways to reduce hallucinations:**\n",
    "- **Lower the temperature** (more determinism). In many APIs the common range is **0.0‚Äì2.0** (sometimes **0.0‚Äì1.0**). **Lower** values ‚Üí more precision/consistency; **higher** values ‚Üí more creativity/variation.\n",
    "- **Provide reliable external context** (RAG: databases, documents, search) instead of letting the model ‚Äúguess‚Äù.\n",
    "- **Request structured outputs** (e.g., validated JSON) and validate the response.\n",
    "- **Ask for sources** or citations when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üëâ Summary\n",
    "\n",
    "- **Context** = everything the model knows to answer (chat history + instructions + external data).\n",
    "- **Context window** = short-term memory (limited by **tokens**).\n",
    "- **Tokens** = the text chunks the model actually processes.\n",
    "- **Prompt** = your instruction to guide the response.\n",
    "- **Hallucinations** = when the model confidently invents things **without real data**; reduce them with **low temperature**, **good context**, and **structured outputs**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
