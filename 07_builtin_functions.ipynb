{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736da1ae",
   "metadata": {},
   "source": [
    "# 08 — Built-in Functions: `.bind()` and `.assign()`\n",
    "\n",
    "This notebook introduces two powerful LCEL helpers used when composing chains:\n",
    "\n",
    "- **`.bind()`** — freeze (attach) provider-specific arguments or generation parameters onto a runnable (usually an LLM) at build time.\n",
    "- **`.assign()`** — add new keys to the **dictionary output** of a runnable (commonly a `RunnableParallel`) without redefining the whole structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fe167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment loaded and model ready.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════╗\n",
    "# ║ Setup: Load environment variables & initialize model ║\n",
    "# ╚══════════════════════════════════════════════════════╝\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# chat_model = ChatGroq(model=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "print(\"✅ Environment loaded and model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cdc95",
   "metadata": {},
   "source": [
    "## `.bind()` — freeze provider/config arguments on a runnable\n",
    "\n",
    "`Runnable.bind(**kwargs)` lets you **inject extra arguments directly into the runnable** (usually a chat model) at chain-construction time. Think of it as *freezing* certain parameters so you don’t have to pass them on every call.\n",
    "\n",
    "**Typical uses:**\n",
    "- Provider-specific features (e.g., OpenAI **tools**, Anthropic **tool use**).\n",
    "- Generation params like `temperature`, `max_tokens`, `stop`, etc.\n",
    "- Low-level control when the high-level helpers are not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5601e",
   "metadata": {},
   "source": [
    "### Example: bind generation parameters\n",
    "\n",
    "We bind temperature and max tokens once, then reuse the model in a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024d6829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL, or LangChain Event Log, is a feature in LangChain that allows users to track and log events during the execution of language model applications. It helps in monitoring, debugging, and analyzing the performance of the application by providing insights into the interactions and decisions made by the model.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer briefly in no more than 2 sentences: {question}\"\n",
    ")\n",
    "\n",
    "# Freeze provider params into the runnable\n",
    "model_tuned = chat_model.bind(temperature=0, max_tokens=120)\n",
    "\n",
    "chain = prompt | model_tuned | StrOutputParser()\n",
    "print(chain.invoke({\"question\": \"What is LCEL in LangChain?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d12356",
   "metadata": {},
   "source": [
    "### Example: tool calling (provider-specific via `.bind()`)\n",
    "\n",
    "You can bind tool specifications directly (low-level) to an OpenAI model. The model may return **tool call instructions** in `additional_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6bb5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model text:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# --- Manual tool definition (JSON schema style) ---\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"City name\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "model_raw = chat_model.bind(tools=tools)\n",
    "\n",
    "resp = model_raw.invoke(\"What's the weather in Madrid?\")\n",
    "print(\"Model output:\\n\", resp.content)\n",
    "print(\"\\nRaw tool_calls (if any):\", getattr(resp, \"tool_calls\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02b5d4",
   "metadata": {},
   "source": [
    "> **Notes**\n",
    "> - `.bind()` returns a **new runnable**; it does not mutate the original. You can reuse the base model.\n",
    "> - You can chain multiple `.bind()` calls:\n",
    ">   ```python\n",
    ">   base = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ">   model = base.bind(temperature=0).bind(max_tokens=100)\n",
    ">   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b22bff",
   "metadata": {},
   "source": [
    "#### Using `.bind_tools()` (high-level helper)\n",
    "\n",
    "You can define tools with the `@tool` decorator and bind them in a provider-agnostic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdd044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@tool\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Return dummy weather (example).\"\"\"\n",
    "    return f\"The weather in {location} is 20° {unit} (demo).\"\n",
    "\n",
    "# 2️⃣ Bind the tool to the model\n",
    "model_with_tools = chat_model.bind_tools([get_current_weather])\n",
    "\n",
    "# 3️⃣ Invoke with a user message\n",
    "resp = model_with_tools.invoke(\"What's the weather in Madrid?\")\n",
    "print(\"Model output:\\n\", resp.content)\n",
    "\n",
    "# 4️⃣ (Optional) Inspect if the model proposed a tool call\n",
    "if hasattr(resp, \"tool_calls\") and resp.tool_calls:\n",
    "    print(\"\\nTool call proposed:\\n\", resp.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24fc179",
   "metadata": {},
   "source": [
    "### Differences `.bind()` vs `.bind_tools()`\n",
    "\n",
    "- **`.bind_tools()`** → high-level, recommended for tool calling. It abstracts provider specifics and uses a more standard interface.\n",
    "- **`.bind()`** → low-level; you pass raw provider arguments (e.g., the exact `tools` schema that OpenAI expects). Useful when you need fine-grained control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c21dd2",
   "metadata": {},
   "source": [
    "---\n",
    "## `.assign()` — add new keys to a dict output\n",
    "\n",
    "`Runnable.assign(**new_keys)` extends the **dictionary output** of a runnable with additional keys computed from the current output.\n",
    "\n",
    "Most commonly you start from a `RunnableParallel` (which already returns a dict) and then **add** fields without redefining the whole mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57facfc9",
   "metadata": {},
   "source": [
    "### Step 1: parallel block with only the original input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    \"original_input\": RunnablePassthrough()\n",
    "})\n",
    "\n",
    "print(chain.invoke(\"whatever\"))  # → {'original_input': 'whatever'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596fa93",
   "metadata": {},
   "source": [
    "### Step 2: add a new key with `.assign()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe54e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def make_uppercase(data: dict) -> str:\n",
    "    return data[\"original_input\"].upper()\n",
    "\n",
    "chain2 = (\n",
    "    RunnableParallel({\"original_input\": RunnablePassthrough()})\n",
    "    .assign(uppercase=RunnableLambda(make_uppercase))\n",
    ")\n",
    "\n",
    "print(chain2.invoke(\"whatever\"))\n",
    "# → {'original_input': 'whatever', 'uppercase': 'WHATEVER'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfef2b5",
   "metadata": {},
   "source": [
    "### Step 3: add more fields progressively\n",
    "You can call `.assign()` multiple times to keep enriching the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed432a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain3 = chain2.assign(\n",
    "    length=RunnableLambda(lambda d: len(d[\"original_input\"]))\n",
    ").assign(\n",
    "    has_space=RunnableLambda(lambda d: \" \" in d[\"original_input\"]) \n",
    ")\n",
    "\n",
    "print(chain3.invoke(\"LangChain rocks!\"))\n",
    "# → {'original_input': 'LangChain rocks!', 'uppercase': 'LANGCHAIN ROCKS!', 'length': 16, 'has_space': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfe3b4",
   "metadata": {},
   "source": [
    "### Example: prepare prompt variables then call a model\n",
    "\n",
    "We first build a dict of variables, then pass it into a prompt → model → parser chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2738e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1) Build the variables dict using parallel + assign\n",
    "vars_chain = (\n",
    "    RunnableParallel({\"question\": RunnablePassthrough()})\n",
    "    .assign(instruction=RunnableLambda(lambda d: \"Answer briefly.\"))\n",
    ")\n",
    "\n",
    "# 2) Create prompt using those variables\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"{instruction}\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# 3) Compose: variables → prompt → model → parser\n",
    "full_chain = vars_chain | prompt | chat_model | StrOutputParser()\n",
    "print(full_chain.invoke(\"What is a Runnable in LangChain?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e87a2",
   "metadata": {},
   "source": [
    "> **Rules of thumb for `.assign()`**\n",
    "> - Use it when you already have a dict output and want to **add derived keys**.\n",
    "> - Great for **progressively enriching** data.\n",
    "> - The function you pass receives the **entire dict so far** (e.g., `{ 'original_input': ... }`).\n",
    "> - `.assign()` only **adds** keys; it doesn’t remove or overwrite existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289ad8e",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- **`.bind()`** freezes provider args or generation params on a runnable (great for tool calling and consistent generation settings).\n",
    "- **`.assign()`** adds new keys to a dict output, perfect for building up prompt variables or enriching intermediate results.\n",
    "- `RunnableParallel` can be expressed in different syntaxes; **a plain dict in a chain is treated as parallel** by LCEL.\n",
    "\n",
    "These tools help you write clean, declarative, and reusable chains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
