{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 05 — Prompt Templates\n",
    "\n",
    "In LangChain, a **Prompt Template** is a predefined text structure that helps you build dynamic prompts. Instead of hardcoding entire prompts every time, you define **templates with variables** — this makes your code cleaner, reusable, and more adaptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad6bbc",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure your `OPENAI_API_KEY` is available (e.g., via environment variables or `.env`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36116165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════╗\n",
    "# ║ Setup: Load environment variables & initialize model ║\n",
    "# ╚══════════════════════════════════════════════════════╝\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load API keys from .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Import your preferred model provider\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize model (OpenAI by default)\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# chat_model = ChatGroq(model=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "print(\"✅ Environment loaded and model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concept",
   "metadata": {},
   "source": [
    "## 🔹 Concept\n",
    "\n",
    "A prompt template contains:\n",
    "- **Fixed text** → content that always appears.\n",
    "- **Variables** → placeholders filled with specific values at runtime.\n",
    "\n",
    "**Example with variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-template",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in history.\n",
      "Answer the following question clearly: Who was Julius Caesar?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an expert in {topic}.\n",
    "Answer the following question clearly: {question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(topic=\"history\", question=\"Who was Julius Caesar?\")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output-example",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "```\n",
    "You are an expert in history.\n",
    "Answer the following question clearly: Who was Julius Caesar?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-template",
   "metadata": {},
   "source": [
    "## 🧠 ChatPromptTemplate\n",
    "\n",
    "For chat models, LangChain provides **ChatPromptTemplate**, which lets you define prompts with roles (`system`, `human`, `ai`) and dynamic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chatprompt-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main goal of the Apollo 11 mission, which took place in July 1969, was to land humans on the Moon and safely return them to Earth. This mission aimed to fulfill President John F. Kennedy's 1961 goal of achieving a manned lunar landing before the end of the decade. Apollo 11 successfully accomplished this objective when astronauts Neil Armstrong and Buzz Aldrin landed on the Moon's surface on July 20, 1969, while Michael Collins remained in lunar orbit. Armstrong's famous words, \"That's one small step for [a] man, one giant leap for mankind,\" marked a historic moment in human space exploration.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a {profession} expert in {topic}.\"),\n",
    "        (\"human\", \"Hello, {profession}, could you answer a question for me?\"),\n",
    "        (\"ai\", \"Of course!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"Scientist\",\n",
    "    topic=\"space exploration\",\n",
    "    user_input=\"What was the main goal of the Apollo 11 mission?\",\n",
    ")\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategies",
   "metadata": {},
   "source": [
    "## 🧩 Basic Prompting Strategies\n",
    "\n",
    "Different prompting strategies give different results. Some are more **creative**, others more **structured**.\n",
    "\n",
    "```markdown\n",
    "* Zero-Shot Prompt: \"Classify the sentiment of this review: ...\"\n",
    "* Few-Shot Prompt: \"Classify the sentiment of this review based on these examples: ...\"\n",
    "* Chain-of-Thought Prompt: \"Classify the sentiment of this review based on these examples and explain the reasoning behind each answer.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "few-shot",
   "metadata": {},
   "source": [
    "## 🎯 Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting means you **show the model a few examples** of how you want it to respond, before asking your real question.\n",
    "\n",
    "This helps guide its reasoning and output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "few-shot-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "# --- Deterministic English→Spanish translation (output-only) ---\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Strong, explicit instruction\n",
    "system_msg = (\n",
    "    \"You are a professional English→Spanish translator. \"\n",
    "    \"Translate the user's message into natural, idiomatic Spanish. \"\n",
    "    \"Output ONLY the translation text. No explanations, no quotes, no preface.\"\n",
    ")\n",
    "\n",
    "# Few-shot examples to anchor behavior\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "    {\"input\": \"Good morning, everyone.\", \"output\": \"Buenos días a todos.\"},\n",
    "    {\"input\": \"How are you?\", \"output\": \"¿Cómo estás?\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    ")\n",
    "\n",
    "few_shot = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt, examples=examples\n",
    ")\n",
    "\n",
    "# Final prompt with instruction + examples + user input\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_msg),\n",
    "        few_shot,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = final_prompt | chat_model | parser\n",
    "\n",
    "print(chain.invoke({\"input\": \"How are you?\"}))\n",
    "# → expected: ¿Cómo estás?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tips",
   "metadata": {},
   "source": [
    "## 🧠 Prompt Design Tips\n",
    "\n",
    "✅ **Be explicit** — clearly tell the model its role and output format.\n",
    "\n",
    "✅ **Use delimiters** — separate context, examples, or data to avoid confusion (e.g., triple backticks ``` or XML-style tags).\n",
    "\n",
    "✅ **Prefer short, clear language** — models understand simpler prompts more reliably.\n",
    "\n",
    "✅ **Always test variations** — small wording changes can drastically affect results.\n",
    "\n",
    "✅ **Keep temperature low** when you need consistency or factual precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- **Prompt templates** = reusable patterns with variables.\n",
    "- **ChatPromptTemplate** = structured multi-message chat prompts.\n",
    "- **FewShotPrompting** = guide the model with examples.\n",
    "- Combine templates + structured prompts to achieve **reliable, consistent** outputs.\n",
    "\n",
    "LangChain makes it easy to **compose** and **reuse** prompts — a key skill when building scalable LLM apps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
