{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fc5d77",
   "metadata": {},
   "source": [
    "# 12 â€” Text Embeddings (Concept & Practice)\n",
    "\n",
    "An **embedding** is a **numerical representation** of text â€” a way to map words, sentences, or documents into vectors (lists of numbers) that capture **semantic meaning**.\n",
    "\n",
    "ğŸ‘‰ The closer two vectors are, the more **similar their meanings**.\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "- Embeddings let models understand similarity **beyond exact wording**.\n",
    "- They are the foundation of **semantic search**, **retrieval**, and **RAG** (Retrieval-Augmented Generation).\n",
    "\n",
    "Examples:\n",
    "- â€œHelloâ€ and â€œHi there!â€ â†’ close vectors.\n",
    "- â€œHelloâ€ and â€œAirplaneâ€ â†’ distant vectors.\n",
    "\n",
    "In short: embeddings allow computers to measure **meaning**, not just text matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179504ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ Setup: Load environment variables & initialize model â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings  # alternative\n",
    "\n",
    "print(\"âœ… Environment loaded and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356f29e",
   "metadata": {},
   "source": [
    "## Generating embeddings for short texts\n",
    "\n",
    "We'll use `OpenAIEmbeddings` to convert short sentences into numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embeddings model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Define a few text fragments\n",
    "chunks_of_text = [\n",
    "    \"Hi there!\",\n",
    "    \"Hello!\",\n",
    "    \"What's your name?\",\n",
    "    \"Bond, James Bond\",\n",
    "    \"Hello Bond!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embeddings_model.embed_documents(chunks_of_text)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions.\")\n",
    "print(f\"First 5 values of the first vector: {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f29aed",
   "metadata": {},
   "source": [
    "### Whatâ€™s happening here\n",
    "\n",
    "- Each sentence becomes a **vector of 1536 dimensions** (specific to OpenAIâ€™s model).\n",
    "- Example:\n",
    "\n",
    "```python\n",
    "[-0.0202, -0.0070, -0.0228, -0.0262, -0.0374, ...]\n",
    "```\n",
    "\n",
    "You donâ€™t interpret each number individually â€” together they encode the **semantic meaning** of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83276334",
   "metadata": {},
   "source": [
    "## Embedding a query (for semantic search)\n",
    "\n",
    "We can embed a **query** and then compare it with our text embeddings to see which chunk is most semantically related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a user query\n",
    "query = \"What was the name mentioned in the conversation?\"\n",
    "embedded_query = embeddings_model.embed_query(query)\n",
    "\n",
    "print(f\"Query vector length: {len(embedded_query)} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d09d2c",
   "metadata": {},
   "source": [
    "In a real RAG system, we would now compute **cosine similarity** between this query vector and the document vectors.\n",
    "\n",
    "The most similar vectors (highest cosine similarity) correspond to the most relevant texts â€” in this case, probably â€œBond, James Bondâ€ or â€œHello Bond!â€."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423a60a",
   "metadata": {},
   "source": [
    "## Why embeddings are powerful\n",
    "\n",
    "- **Language-independent** â€” similar ideas cluster together even across languages.\n",
    "- **Efficient** â€” numeric vectors are lightweight and fast to compare.\n",
    "- **Versatile** â€” used for clustering, retrieval, recommendations, semantic search, etc.\n",
    "\n",
    "In RAG pipelines, embeddings are the *bridge* between raw text and vector databases (like FAISS, Chroma, or Milvus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03871063",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Embeddings convert text into **meaningful numeric vectors**.\n",
    "- Similar meanings â†’ vectors close together.\n",
    "- They power **semantic search, RAG, and intelligent retrieval**.\n",
    "- Next, weâ€™ll see how to **store** these embeddings efficiently in a **vector database** for retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
