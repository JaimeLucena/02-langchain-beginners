{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 — Chains\n",
        "\n",
        "In LangChain, a **Chain** is a sequence of steps (called *runnables*) that execute in order.\n",
        "\n",
        "Each step takes an input, processes it, and passes the result to the next one.\n",
        "\n",
        "👉 With this you can combine: **prompts → models → parsers → custom logic**.\n",
        "\n",
        "Think of it as a *pipeline* where each stage has a clear responsibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ╔══════════════════════════════════════════════════════╗\n",
        "# ║ Setup: Load environment variables & initialize model ║\n",
        "# ╚══════════════════════════════════════════════════════╝\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain_groq import ChatGroq\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "# chat_model = ChatGroq(model=\"llama-3.1-70b-versatile\")\n",
        "\n",
        "print(\"✅ Environment loaded and model ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 What is a Chain?\n",
        "\n",
        "A **Chain** in LangChain is a simple but powerful way to connect multiple components.\n",
        "\n",
        "Each part (called a *runnable*) performs one operation — like building a prompt, generating text, or parsing the output.\n",
        "\n",
        "Together, they create a flow from **input → output**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ Example: Simple Chain\n",
        "\n",
        "Let's build a very basic Chain using three components:\n",
        "\n",
        "1. A **PromptTemplate** (creates the message)\n",
        "2. A **ChatModel** (generates a response)\n",
        "3. A **Parser** (cleans the output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt structure\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Tell me a surprising fact about {scientist}.\"\n",
        ")\n",
        "\n",
        "# Combine prompt + model + parser into a chain\n",
        "chain = prompt | chat_model | StrOutputParser()\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"scientist\": \"Marie Curie\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔍 Step-by-step explanation\n",
        "\n",
        "1. **ChatPromptTemplate**\n",
        "   - Defines the text pattern: `Tell me a surprising fact about {scientist}`.\n",
        "   - Replaces `{scientist}` with the input value (`Marie Curie`).\n",
        "   - Output → structured prompt ready for the model.\n",
        "\n",
        "   Example:\n",
        "\n",
        "   *“Tell me a surprising fact about Marie Curie.”*\n",
        "\n",
        "---\n",
        "\n",
        "2. **ChatOpenAI (the LLM)**\n",
        "   - Receives the prompt.\n",
        "   - Generates a natural language response.\n",
        "   - Output → structured chat message like:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'role': 'assistant',\n",
        "  'content': 'Marie Curie was the first person to win Nobel Prizes in two different sciences — Physics and Chemistry.'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "3. **StrOutputParser**\n",
        "   - Converts structured output into plain text.\n",
        "   - Extracts only the `content` field.\n",
        "   - Result → clean string ready to display.\n",
        "\n",
        "   Example:\n",
        "\n",
        "   *“Marie Curie was the first person to win Nobel Prizes in two different sciences — Physics and Chemistry.”*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧩 What does `StrOutputParser()` do?\n",
        "\n",
        "- If the model returns plain text → it passes it through.\n",
        "- If the model returns a structured message (like a ChatModel) → it extracts the main content.\n",
        "- Final result → always a **clean string**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Visual representation of this chain\n",
        "\n",
        "**Input:**\n",
        "```python\n",
        "{\"scientist\": \"Marie Curie\"}\n",
        "```\n",
        "\n",
        "⬇️ **PromptTemplate:**\n",
        "```\n",
        "Tell me a surprising fact about Marie Curie.\n",
        "```\n",
        "\n",
        "⬇️ **ChatOpenAI:**\n",
        "```\n",
        "{'role': 'assistant', 'content': '...'}\n",
        "```\n",
        "\n",
        "⬇️ **StrOutputParser:**\n",
        "```\n",
        "...plain text response...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 💡 Why Chains matter\n",
        "\n",
        "- **Modularity:** Each block does one thing well (prompt, model, parser).\n",
        "- **Reusability:** You can swap a single block (e.g., a parser) without changing the others.\n",
        "- **Composition:** Combine small chains into larger workflows.\n",
        "- **Maintainability:** Easier to debug, extend, and explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Summary\n",
        "\n",
        "A **Chain** is an ordered sequence of processing steps.\n",
        "\n",
        "In this example:\n",
        "\n",
        "1. The prompt is built with a variable (`scientist`).\n",
        "2. The model generates a response.\n",
        "3. The parser returns the result as plain text.\n",
        "\n",
        "This is the foundation of how more complex LangChain pipelines work.\n",
        "\n",
        "In the next notebook, we’ll explore **different types of Chains** and how to combine them effectively."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}