{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# 03 ‚Äî Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "RAG exists to overcome **context window limits**. You cannot ask an LLM to internally read 100+ pages if it doesn't fit into its short-term memory (the context window). RAG retrieves only the **relevant** pieces and feeds them to the model at generation time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "why-options",
      "metadata": {},
      "source": [
        "## Why RAG? Options to use your data with LLMs\n",
        "\n",
        "- **Train an LLM from scratch with your data.**  \n",
        "  *Extremely expensive.* Practically infeasible for most teams.\n",
        "\n",
        "- **Fine-tune an existing LLM with your data.**  \n",
        "  *Expensive and technically complex.* Often overkill and hard to maintain.\n",
        "\n",
        "- **RAG (Retrieval-Augmented Generation).**  \n",
        "  Split your data into small chunks, embed them into vectors (\"embeddings\"), and store them in a **vector database**. At query time, retrieve the most similar chunks and pass them to the LLM.\n",
        "\n",
        "**In practice, most LLM applications today use RAG.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "roles",
      "metadata": {},
      "source": [
        "## Division of responsibilities in RAG\n",
        "\n",
        "- **Language generation** comes from the **foundation LLM**.\n",
        "- **Knowledge representation** comes from the **vector database**.\n",
        "\n",
        "In other words:\n",
        "- The foundation LLM behaves like someone who can *speak and reason* but doesn't know your private data.\n",
        "- The vector database behaves like your *domain knowledge*, providing grounded facts on demand."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "how-it-works",
      "metadata": {},
      "source": [
        "## How a RAG app works\n",
        "\n",
        "1. **Question** ‚Äî The user asks a question.\n",
        "2. **Retrieval** ‚Äî A *retriever* searches an indexed document set (PDFs, databases, websites, etc.) and returns the most relevant chunks.\n",
        "3. **Augmented generation** ‚Äî The LLM generates an answer **using** those retrieved chunks as context.\n",
        "4. **Result** ‚Äî The app returns a new answer synthesized from the retrieved information (not just copy-paste).\n",
        "\n",
        "```\n",
        "User Question ‚Üí Retriever (Vector DB) ‚Üí Context Chunks ‚Üí LLM ‚Üí Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "why-useful",
      "metadata": {},
      "source": [
        "## Why it's useful\n",
        "\n",
        "- **Accuracy & relevance** ‚Äî Answers are grounded in retrieved sources.\n",
        "- **Efficiency** ‚Äî Combines search + generation in a single flow.\n",
        "- **Versatility** ‚Äî Works for study aids, research assistants, enterprise knowledge, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "costs",
      "metadata": {},
      "source": [
        "## RAG helps reduce cost\n",
        "\n",
        "- **Selective queries** ‚Äî Retrieve relevant snippets first; the LLM sees **only** the filtered context.\n",
        "- **Lower compute** ‚Äî Less text for the model to process ‚Üí faster and cheaper.\n",
        "\n",
        "üëâ In essence, RAG ensures that the most expensive resource (LLM compute) is used **only** where it adds value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "privacy",
      "metadata": {},
      "source": [
        "## Privacy in RAG\n",
        "\n",
        "- Data is provided to the LLM **only at generation time** ‚Äî not for training.\n",
        "- Therefore, your information is **not stored inside the model**; it is shown temporarily as context.\n",
        "- After answering, the LLM **does not remember** those inputs.\n",
        "\n",
        "‚ö†Ô∏è In production-grade RAG, carefully classify stored data and apply strong security controls for sensitive content."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indexing",
      "metadata": {},
      "source": [
        "## Indexing pipeline (offline)\n",
        "\n",
        "1. **Load**: Ingest documents (PDFs, HTML, databases, etc.).\n",
        "2. **Split**: Chunk the text with robust splitters (e.g., `RecursiveCharacterTextSplitter`).\n",
        "3. **Embed**: Convert chunks into vectors using an embedding model.\n",
        "4. **Store**: Persist vectors into a **vector store** (FAISS, Chroma, Pinecone, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "retrieval-flow",
      "metadata": {},
      "source": [
        "## Retrieval + generation flow (online)\n",
        "\n",
        "1. Receive a **user question**.\n",
        "2. **Retrieve** top-*k* most similar chunks from the vector store.\n",
        "3. Build a **prompt** that includes the retrieved context + the user question.\n",
        "4. Call the **LLM** to generate the final answer.\n",
        "5. (Optional) **Cite sources**, validate JSON, apply guardrails, or cache results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "temperature",
      "metadata": {},
      "source": [
        "### Controlling hallucinations with temperature\n",
        "\n",
        "- Lower temperature ‚Üí more deterministic, fewer hallucinations.\n",
        "- Higher temperature ‚Üí more creative, but riskier.\n",
        "\n",
        "```python\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "```\n",
        "Combine **low temperature** with **good retrieval** and **structured prompts** for best grounding."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
